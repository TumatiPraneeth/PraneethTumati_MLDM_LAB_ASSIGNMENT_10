{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Praneeth_Tumati_Lab_Assignment_10.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM6mXGWK+z/UGhGeGYGfL7w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TumatiPraneeth/PraneethTumati_MLDM_LAB_ASSIGNMENT_10/blob/master/Praneeth_Tumati_Lab_Assignment_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cyy-14q-mE7n",
        "colab_type": "text"
      },
      "source": [
        "**Name:**Tumati Naga Praneeth\n",
        "**enrollment number:**1700282C203\n",
        "**CLASS:**CSE-2\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJ6ney--mEKi",
        "colab_type": "code",
        "outputId": "daab333f-9970-47d9-b6da-efb6b9a34691",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#question1\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "moves = [1,2,3,4,5,6]\n",
        "prob = [0.2,0.3,0.2,0.1,0.1,0.1]\n",
        "\n",
        "def rollDice():\n",
        "    current_move = np.random.choice(a= moves , p = prob)\n",
        "    return int(current_move) \n",
        "\n",
        "Iterations = 10000\n",
        "down_movement = [1,2]\n",
        "up_movement = [3,4,5]\n",
        "successful_iterations = 0\n",
        "selected6 = 0\n",
        "for i in range (0,Iterations): \n",
        "    total_steps = 250\n",
        "    current_position = 0\n",
        "    selected6 = 0\n",
        "    completedMoves = 0\n",
        "    while completedMoves < total_steps:\n",
        "        completedMoves +=1\n",
        "        if(current_position > 60):\n",
        "            successful_iterations+=1\n",
        "            break\n",
        "        current_movement = rollDice()\n",
        "        if(current_movement in  down_movement):\n",
        "            current_position -=1\n",
        "        elif current_movement in up_movement :\n",
        "            current_position +=1    \n",
        "        else  :#current movement  = 6\n",
        "            completedMoves -=1\n",
        "            current_movement = rollDice()\n",
        "            current_position += current_movement\n",
        "        \n",
        "probabilty_of_reaching = successful_iterations/Iterations\n",
        "print(successful_iterations)\n",
        "print(probabilty_of_reaching)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4296\n",
            "0.4296\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPhuV9FGohZd",
        "colab_type": "text"
      },
      "source": [
        "Problem 2)Randomly generated data for Multiple Linear Regression,Logistic Regression,K-means Clustering :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRK0Gb1Qog3_",
        "colab_type": "code",
        "outputId": "5af4abdf-f809-4a57-d417-4cce1ebc8e8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        }
      },
      "source": [
        "#question2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy\n",
        "import random\n",
        "from scipy.stats import norm\n",
        "random.seed(1)\n",
        "n_features_of_Multiple_linear = 4 #declaring the number of features required for multiple linear.\n",
        "\n",
        "X = []\n",
        "\n",
        "for i in range(n_features_of_Multiple_linear):\n",
        "  X_i = scipy.stats.norm.rvs(0, 1, 100)\n",
        "  X.append(X_i)# creating the values to be stored in X.\n",
        "eps = scipy.stats.norm.rvs(0, 0.25,100)\n",
        "y = 1 + (0.5 * X[0]) + eps + (0.4 * X[1]) + (0.3 * X[2]) + (0.5 * X[3])\n",
        "data_multiplelinear = {'X0': X[0],'X1':X[1],'X2':X[2],'X3':X[3],'Y': y }\n",
        "df = pd.DataFrame(data_multiplelinear)#dataframe created.\n",
        "print(df.head())\n",
        "print(df.tail())\n",
        "print(df.info())\n",
        "print(df.describe())\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "         X0        X1        X2        X3         Y\n",
            "0  1.115776  0.442165  2.023871 -0.956984  1.936260\n",
            "1  0.873560 -1.799875 -0.252810  0.349109  0.813800\n",
            "2 -1.270745 -0.050980 -0.601719  0.006422  0.277504\n",
            "3 -1.005667 -0.696950 -2.196211  0.305718 -0.240504\n",
            "4 -0.277740  1.094075  0.601929 -0.233852  1.149810\n",
            "          X0        X1        X2        X3         Y\n",
            "95  0.783093 -0.520701  0.230284 -0.415045  0.863528\n",
            "96  0.430597 -0.238881  0.666279  1.936787  2.094166\n",
            "97  0.655682  0.184762  0.607088 -0.374511  1.301554\n",
            "98 -0.736831 -0.826871 -0.813514 -1.302334 -0.542288\n",
            "99 -0.784254 -0.061177  0.333634  0.181559  0.895314\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100 entries, 0 to 99\n",
            "Data columns (total 5 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   X0      100 non-null    float64\n",
            " 1   X1      100 non-null    float64\n",
            " 2   X2      100 non-null    float64\n",
            " 3   X3      100 non-null    float64\n",
            " 4   Y       100 non-null    float64\n",
            "dtypes: float64(5)\n",
            "memory usage: 4.0 KB\n",
            "None\n",
            "               X0          X1          X2          X3           Y\n",
            "count  100.000000  100.000000  100.000000  100.000000  100.000000\n",
            "mean     0.107522   -0.131259   -0.084836    0.211785    1.119433\n",
            "std      1.089243    1.034280    0.960887    1.003462    0.991200\n",
            "min     -2.594025   -2.522783   -2.671070   -2.101229   -1.523483\n",
            "25%     -0.622268   -0.784104   -0.602102   -0.443492    0.454866\n",
            "50%      0.126201   -0.181720   -0.062108    0.079269    1.091417\n",
            "75%      0.772626    0.586326    0.603219    0.766914    1.764315\n",
            "max      2.695134    2.970286    2.528562    2.476213    3.683486\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TviJr__-y-P5",
        "colab_type": "code",
        "outputId": "fd855146-d8f1-43b3-800c-aedd120b6b68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        }
      },
      "source": [
        "n_features_of_logistic_regression = 4#logistic model Feature required\n",
        "X = []\n",
        "for i in range(n_features_of_logistic_regression):\n",
        "  X_i = scipy.stats.norm.rvs(0, 1, 100)\n",
        "  X.append(X_i)\n",
        "a1 = (np.exp(1 + (0.5 * X[0]) + (0.4 * X[1]) + (0.3 * X[2]) + (0.5 * X[3]))/(1 + np.exp(1 + (0.5 * X[0]) + (0.4 * X[1]) + (0.3 * X[2]) + (0.5 * X[3]))))\n",
        "y1 = []\n",
        "for i in a1:\n",
        "  if (i>=0.5):\n",
        "    y1.append(1)\n",
        "  else:\n",
        "    y1.append(0)\n",
        "data_lr = {'X0': X[0],'X1':X[1],'X2':X[2],'X3':X[3],'Y': y1 }\n",
        "df1 = pd.DataFrame(data_lr)\n",
        "print(df.head())\n",
        "print(df.tail())\n",
        "print(df.info())\n",
        "print(df.describe())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "         X0        X1        X2        X3         Y\n",
            "0  1.115776  0.442165  2.023871 -0.956984  1.936260\n",
            "1  0.873560 -1.799875 -0.252810  0.349109  0.813800\n",
            "2 -1.270745 -0.050980 -0.601719  0.006422  0.277504\n",
            "3 -1.005667 -0.696950 -2.196211  0.305718 -0.240504\n",
            "4 -0.277740  1.094075  0.601929 -0.233852  1.149810\n",
            "          X0        X1        X2        X3         Y\n",
            "95  0.783093 -0.520701  0.230284 -0.415045  0.863528\n",
            "96  0.430597 -0.238881  0.666279  1.936787  2.094166\n",
            "97  0.655682  0.184762  0.607088 -0.374511  1.301554\n",
            "98 -0.736831 -0.826871 -0.813514 -1.302334 -0.542288\n",
            "99 -0.784254 -0.061177  0.333634  0.181559  0.895314\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100 entries, 0 to 99\n",
            "Data columns (total 5 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   X0      100 non-null    float64\n",
            " 1   X1      100 non-null    float64\n",
            " 2   X2      100 non-null    float64\n",
            " 3   X3      100 non-null    float64\n",
            " 4   Y       100 non-null    float64\n",
            "dtypes: float64(5)\n",
            "memory usage: 4.0 KB\n",
            "None\n",
            "               X0          X1          X2          X3           Y\n",
            "count  100.000000  100.000000  100.000000  100.000000  100.000000\n",
            "mean     0.107522   -0.131259   -0.084836    0.211785    1.119433\n",
            "std      1.089243    1.034280    0.960887    1.003462    0.991200\n",
            "min     -2.594025   -2.522783   -2.671070   -2.101229   -1.523483\n",
            "25%     -0.622268   -0.784104   -0.602102   -0.443492    0.454866\n",
            "50%      0.126201   -0.181720   -0.062108    0.079269    1.091417\n",
            "75%      0.772626    0.586326    0.603219    0.766914    1.764315\n",
            "max      2.695134    2.970286    2.528562    2.476213    3.683486\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGs0AUNw3QAP",
        "colab_type": "code",
        "outputId": "961390d9-183b-4750-a0ec-c09787a7e53f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 843
        }
      },
      "source": [
        "#Random data for clustering (k)\n",
        "X_p= -2 * np.random.rand(100,2)\n",
        "X_q = 1 + 2 * np.random.rand(50,2)\n",
        "X_p[50:100, :] = X_q\n",
        "plt.scatter(X_p[ : , 0], X_p[ :, 1], s = 50)\n",
        "plt.show()\n",
        "data_kmeans = {'X0': X_a[:,0],'X1':X_a[:,1]}\n",
        "df3 = pd.DataFrame(data_kmeans)\n",
        "print(df.head())\n",
        "print(df.tail())\n",
        "print(df.info())\n",
        "print(df.describe())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "         X0        X1        X2        X3         Y\n",
            "0  1.115776  0.442165  2.023871 -0.956984  1.936260\n",
            "1  0.873560 -1.799875 -0.252810  0.349109  0.813800\n",
            "2 -1.270745 -0.050980 -0.601719  0.006422  0.277504\n",
            "3 -1.005667 -0.696950 -2.196211  0.305718 -0.240504\n",
            "4 -0.277740  1.094075  0.601929 -0.233852  1.149810\n",
            "          X0        X1        X2        X3         Y\n",
            "95  0.783093 -0.520701  0.230284 -0.415045  0.863528\n",
            "96  0.430597 -0.238881  0.666279  1.936787  2.094166\n",
            "97  0.655682  0.184762  0.607088 -0.374511  1.301554\n",
            "98 -0.736831 -0.826871 -0.813514 -1.302334 -0.542288\n",
            "99 -0.784254 -0.061177  0.333634  0.181559  0.895314\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100 entries, 0 to 99\n",
            "Data columns (total 5 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   X0      100 non-null    float64\n",
            " 1   X1      100 non-null    float64\n",
            " 2   X2      100 non-null    float64\n",
            " 3   X3      100 non-null    float64\n",
            " 4   Y       100 non-null    float64\n",
            "dtypes: float64(5)\n",
            "memory usage: 4.0 KB\n",
            "None\n",
            "               X0          X1          X2          X3           Y\n",
            "count  100.000000  100.000000  100.000000  100.000000  100.000000\n",
            "mean     0.107522   -0.131259   -0.084836    0.211785    1.119433\n",
            "std      1.089243    1.034280    0.960887    1.003462    0.991200\n",
            "min     -2.594025   -2.522783   -2.671070   -2.101229   -1.523483\n",
            "25%     -0.622268   -0.784104   -0.602102   -0.443492    0.454866\n",
            "50%      0.126201   -0.181720   -0.062108    0.079269    1.091417\n",
            "75%      0.772626    0.586326    0.603219    0.766914    1.764315\n",
            "max      2.695134    2.970286    2.528562    2.476213    3.683486\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmFyhNem479O",
        "colab_type": "text"
      },
      "source": [
        "**Problem3:**Implementation of Linear Regression using Gradient Descent,Logistic Regression using Gradient Descent, Linear Regression with L1 and L2 Regularization, Logistic Regression with  L1 and L2 Regularization,K-Means"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z317_HtK47kg",
        "colab_type": "code",
        "outputId": "9973c883-3a4d-478d-e702-48fda5d8ce12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#question3\n",
        "#linear regression using gradient descent\n",
        "print(\"linear regrission parameters using gradient descent are:\" )\n",
        "X = df.iloc[:,0].values\n",
        "y = df.iloc[:,4].values\n",
        "b1 = 0\n",
        "b0 = 0\n",
        "l = 0.001\n",
        "epochs = 100\n",
        " \n",
        "n = float(len(X))\n",
        "for i in range(epochs):\n",
        "  y_p = b1*X + b0\n",
        "  loss = np.sum(y_p - y1)**2\n",
        "  d1 = (-2/n) * sum(X * (y - y_p))\n",
        "  d0 = (-2/n) * sum(y - y_p)\n",
        "  b1 = b1 - (l*d1)\n",
        "  b0 = b0 - (l*d0)\n",
        "              \n",
        "print(b1,b0)#b1,b0 are the predicted constants to the equation"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "linear regrission parameters using gradient descent are:\n",
            "0.12695184597757767 0.20177911427018505\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIqhjH7a8YBD",
        "colab_type": "code",
        "outputId": "d129831f-5503-4978-fdce-5fbe72310d5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "#logistic regression using gradient descent\n",
        "print(\"from logistic using gradient descent are\")\n",
        "X1 = df1.iloc[:,0:4].values\n",
        "y1 = df1.iloc[:,4].values\n",
        "def sigmoid(Z):\n",
        "  return 1 /(1+np.exp(-Z))\n",
        "def loss(y1,y_hat):\n",
        "  return -np.mean(y1*np.log(y_hat) + (1-y1)*(np.log(1-y_hat)))\n",
        "W = np.zeros((4,1))\n",
        "b = np.zeros((1,1))\n",
        "m = len(y1)\n",
        "lr = 0.001\n",
        "for epoch in range(1000):\n",
        "  Z = np.matmul(X1,W)+b\n",
        "  A = sigmoid(Z)\n",
        "  logistic_loss = loss(y1,A)\n",
        "  dz = A - y1\n",
        "  dw = 1/m * np.matmul(X1.T,dz)\n",
        "  db = np.sum(dz)\n",
        "  W = W - lr*dw\n",
        "  b = b - lr*db\n",
        "  if epoch % 100 == 0:\n",
        "    print(logistic_loss)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "from logistic using gradient descent are\n",
            "0.6931471805599453\n",
            "0.25318648962897633\n",
            "0.25273961767072667\n",
            "0.25229833793635603\n",
            "0.2518626607370973\n",
            "0.2514325980547191\n",
            "0.2510081632298061\n",
            "0.2505893706440734\n",
            "0.2501762353979384\n",
            "0.2497687729847441\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9FwNC9SBZKb",
        "colab_type": "text"
      },
      "source": [
        "**Linear Regression using L1 and L2 regularization:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eIp5YtzBO0m",
        "colab_type": "code",
        "outputId": "cd835c37-97ff-4d04-ecde-0bcc1d7d9f8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "print(\"Linear regression using L1 regularisation\")\n",
        "X = df.iloc[:,0].values\n",
        "y = df.iloc[:,4].values\n",
        "b1 = 0\n",
        "b0 = 0\n",
        "l = 0.001\n",
        "epochs = 100\n",
        "lam = 0.1\n",
        "n = float(len(X))\n",
        "for i in range(epochs):\n",
        "  y_p = b1*X + b0\n",
        "  loss = np.sum(y_p - y1)**2 + (lam * b1)\n",
        "  d1 = (-2/n) * sum(X * (y - y_p)) + lam\n",
        "  d0 = (-2/n) * sum(y - y_p)\n",
        "  b1 = b1 - (l*d1)\n",
        "  b0 = b0 - (l*d0)\n",
        "print(b1,b0,\"are the parameters obtained of L1 regularisation\")\n",
        "print()\n",
        "\n",
        "print(\"Linear regression using L2 regularisation\")\n",
        "X = df.iloc[:,0].values\n",
        "#print(X)\n",
        "y = df.iloc[:,4].values\n",
        "b1 = 0\n",
        "b0 = 0\n",
        "l = 0.001\n",
        "epochs = 100\n",
        "lam = 0.1\n",
        "n = float(len(X))\n",
        "for i in range(epochs):\n",
        "  y_p = b1*X + b0\n",
        "  loss = np.sum(y_p - y1)**2 + ((lam/2) * b1)\n",
        "  d1 = (-2/n) * sum(X * (y - y_p)) + (lam *b1)\n",
        "  d0 = (-2/n) * sum(y - y_p)\n",
        "  b1 = b1 - (l*d1)\n",
        "  b0 = b0 - (l*d0)\n",
        "print(b1,b0,\"are the parameters obtained of L2 regularisation\")\n",
        "print()\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linear regression using L1 regularisation\n",
            "0.11803950017078525 0.2018715072834019 are the parameters obtained of L1 regularisation\n",
            "\n",
            "Linear regression using L2 regularisation\n",
            "0.12634581363879765 0.2017833476416372 are the parameters obtained of L2 regularisation\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGUQAkY-CRZQ",
        "colab_type": "text"
      },
      "source": [
        "**Logistic Regression using L1 and L2 Regularization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5E6xn4YCRqi",
        "colab_type": "code",
        "outputId": "0b217471-592b-45d3-a00b-d7190aed53fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "print(\"Logistic regression using L1 regularisation\")\n",
        "X1 = df1.iloc[:,0:4].values\n",
        "y1 = df1.iloc[:,4].values\n",
        "lam = 0.1\n",
        "def sigmoid(Z):\n",
        "  return 1 /(1+np.exp(-Z))\n",
        "\n",
        "def loss(y1,y_hat):\n",
        "  return -np.mean(y1*np.log(y_hat) + (1-y1)*(np.log(1-y_hat))) + (lam * (np.sum(W)))\n",
        "\n",
        "W = np.zeros((4,1))\n",
        "b = np.zeros((1,1))\n",
        "\n",
        "m = len(y1)\n",
        "lr = 0.001\n",
        "for epoch in range(1000):\n",
        "  Z = np.matmul(X1,W)+b\n",
        "  A = sigmoid(Z)\n",
        "  logistic_loss = loss(y1,A)\n",
        "  dz = A - y1\n",
        "  dw = 1/m * np.matmul(X1.T,dz) + lam\n",
        "  db = np.sum(dz)\n",
        "\n",
        "  W = W - lr*dw\n",
        "  b = b - lr*db\n",
        "\n",
        "  if epoch % 100 == 0:\n",
        "    print(\"logistic loss occured\",logistic_loss)\n",
        "\n",
        "print()\n",
        "print()\n",
        "print()\n",
        "print(\"Logistic regression using L2 regularisation\")\n",
        "X1 = df1.iloc[:,0:4].values\n",
        "y1 = df1.iloc[:,4].values\n",
        "lam = 0.1\n",
        "def sigmoid(Z):\n",
        "  return 1 /(1+np.exp(-Z))\n",
        "\n",
        "def loss(y1,y_hat):\n",
        "  return -np.mean(y1*np.log(y_hat) + (1-y1)*(np.log(1-y_hat))) + (lam * (np.sum(np.square(W))))\n",
        "\n",
        "W = np.zeros((4,1))\n",
        "b = np.zeros((1,1))\n",
        "\n",
        "m = len(y1)\n",
        "lr = 0.001\n",
        "for epoch in range(1000):\n",
        "  Z = np.matmul(X1,W)+b\n",
        "  A = sigmoid(Z)\n",
        "  logistic_loss = loss(y1,A)\n",
        "  dz = A - y1\n",
        "  dw = 1/m * np.matmul(X1.T,dz) + lam * W\n",
        "  db = np.sum(dz)\n",
        "\n",
        "  W = W - lr*dw\n",
        "  b = b - lr*db\n",
        "\n",
        "  if epoch % 100 == 0:\n",
        "    print(\"Logistic loss occured:\",logistic_loss,)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logistic regression using L1 regularisation\n",
            "logistic loss occured 0.6931471805599453\n",
            "logistic loss occured -0.14457229268292582\n",
            "logistic loss occured -0.5420035039092344\n",
            "logistic loss occured -0.9374171978427812\n",
            "logistic loss occured -1.3308197960917192\n",
            "logistic loss occured -1.7222176376324925\n",
            "logistic loss occured -2.1116170128053704\n",
            "logistic loss occured -2.499024203797419\n",
            "logistic loss occured -2.8844455312588626\n",
            "logistic loss occured -3.2678874064951224\n",
            "\n",
            "\n",
            "\n",
            "Logistic regression using L2 regularisation\n",
            "Logistic loss occured: 0.6931471805599453\n",
            "Logistic loss occured: 0.25363678027137815\n",
            "Logistic loss occured: 0.25451186072548476\n",
            "Logistic loss occured: 0.2562219952561201\n",
            "Logistic loss occured: 0.25872622868613393\n",
            "Logistic loss occured: 0.26198485103762775\n",
            "Logistic loss occured: 0.2659593625295551\n",
            "Logistic loss occured: 0.270612441321454\n",
            "Logistic loss occured: 0.2759079137568816\n",
            "Logistic loss occured: 0.2818107268670077\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pd8XUX2kDPna",
        "colab_type": "text"
      },
      "source": [
        "**K-means Clustering:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxsMqet5DTYY",
        "colab_type": "code",
        "outputId": "09fce9c7-e5f8-4c53-8f09-96a0dd12520e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "class K_Means:\n",
        "    def __init__(self, k=2, tol=0.001, max_iter=300):\n",
        "        self.k = k\n",
        "        self.tol = tol\n",
        "        self.max_iter = max_iter\n",
        "\n",
        "    def fit(self,data):\n",
        "\n",
        "        self.centroids = {}\n",
        "\n",
        "        for i in range(self.k):\n",
        "            self.centroids[i] = data[i]\n",
        "\n",
        "        for i in range(self.max_iter):\n",
        "            self.classifications = {}\n",
        "\n",
        "            for i in range(self.k):\n",
        "                self.classifications[i] = []\n",
        "\n",
        "            for featureset in X:\n",
        "                distances = [np.linalg.norm(featureset-self.centroids[centroid]) for centroid in self.centroids]\n",
        "                classification = distances.index(min(distances))\n",
        "                self.classifications[classification].append(featureset)\n",
        "\n",
        "            prev_centroids = dict(self.centroids)\n",
        "\n",
        "            for classification in self.classifications:\n",
        "                self.centroids[classification] = np.average(self.classifications[classification],axis=0)\n",
        "\n",
        "            optimized = True\n",
        "\n",
        "            for c in self.centroids:\n",
        "                original_centroid = prev_centroids[c]\n",
        "                current_centroid = self.centroids[c]\n",
        "                if np.sum((current_centroid-original_centroid)/original_centroid*100.0) > self.tol:\n",
        "                    print(np.sum((current_centroid-original_centroid)/original_centroid*100.0))\n",
        "                    optimized = False\n",
        "\n",
        "            if optimized:\n",
        "                break\n",
        "\n",
        "    def predict(self,data):\n",
        "        distances = [np.linalg.norm(data-self.centroids[centroid]) for centroid in self.centroids]\n",
        "        classification = distances.index(min(distances))\n",
        "        return classification\n",
        "        \n",
        "colors = 10*[\"g\",\"r\",\"c\",\"b\",\"k\"]\n",
        "X = df3.iloc[:,0:2].values\n",
        "clf = K_Means()\n",
        "clf.fit(X)\n",
        "\n",
        "for centroid in clf.centroids:\n",
        "    plt.scatter(clf.centroids[centroid][0], clf.centroids[centroid][1],\n",
        "                marker=\"o\", color=\"k\", s=150, linewidths=5)\n",
        "\n",
        "for classification in clf.classifications:\n",
        "    color = colors[classification]\n",
        "    for featureset in clf.classifications[classification]:\n",
        "        plt.scatter(featureset[0], featureset[1], marker=\"x\", color=color, s=150, linewidths=5)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nh_LC8oGECNp",
        "colab_type": "text"
      },
      "source": [
        "**Using OOPS Concept:**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxPZOQPfEMy9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#question4\n",
        "#linear Regression using oops Concept:\n",
        "import numpy as np\n",
        "\n",
        "class LinearRegressionModel():\n",
        "\n",
        "    def __init__(self, dataset, learning_rate, num_iterations):\n",
        "        self.dataset = np.array(dataset)\n",
        "        self.b = 0  \n",
        "        self.m = 0  \n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_iterations = num_iterations\n",
        "        self.M = len(self.dataset)\n",
        "        self.total_error = 0\n",
        "\n",
        "    def apply_gradient_descent(self):\n",
        "        for i in range(self.num_iterations):\n",
        "            self.do_gradient_step()\n",
        "\n",
        "    def do_gradient_step(self):\n",
        "        b_summation = 0\n",
        "        m_summation = 0\n",
        "        for i in range(self.M):\n",
        "            x_value = self.dataset[i, 0]\n",
        "            y_value = self.dataset[i, 1]\n",
        "            b_summation += (((self.m * x_value) + self.b) - y_value) \n",
        "            m_summation += (((self.m * x_value) + self.b) - y_value) * x_value\n",
        "        self.b = self.b - (self.learning_rate * (1/self.M) * b_summation)\n",
        "        self.m = self.m - (self.learning_rate * (1/self.M) * m_summation)\n",
        "      \n",
        "    def compute_error(self):\n",
        "        for i in range(self.M):\n",
        "            x_value = self.dataset[i, 0]\n",
        "            y_value = self.dataset[i, 1]\n",
        "            self.total_error += ((self.m * x_value) + self.b) - y_value\n",
        "        return self.total_error\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"Results: b: {}, m: {}, Final Total error: {}\".format(round(self.b, 2), round(self.m, 2), round(self.compute_error(), 2))\n",
        "\n",
        "    def get_prediction_based_on(self, x):\n",
        "        return round(float((self.m * x) + self.b), 2) # Type: Numpy float.\n",
        "\n",
        "def main():\n",
        "    school_dataset = np.genfromtxt(DATASET_PATH, delimiter=\",\")\n",
        "    lr = LinearRegressionModel(school_dataset, 0.0001, 1000)\n",
        "    lr.apply_gradient_descent()\n",
        "    no_of_hours = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
        "    for hour in hours:\n",
        "        print(\"Worked hours {} hours and got {} bonus.\".format(hour, lr.get_prediction_based_on(hour)))\n",
        "    print(lr)\n",
        "\n",
        "if __name__ == \"__main__\": main()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvQcmjlbH8Aq",
        "colab_type": "text"
      },
      "source": [
        "Logistic regression:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2f2HBqF5G6Sa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LogisticRegression:\n",
        "  def __init__(self, learning_rate, num_iters, fit_intercept = True, verbose = False):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.num_iters = num_iters\n",
        "    self.fit_intercept = fit_intercept\n",
        "    self.verbose = verbose\n",
        "  def __add_intercept(self, X):\n",
        "    intercept = np.ones((X.shape[0],1))\n",
        "    return np.concatenate((intercept,X),axis=1)\n",
        "  def __sigmoid(self,z):\n",
        "    return 1/(1+np.exp(-z))\n",
        "  def __loss(self, h, y):\n",
        "    return (-y * np.log(h) - (1-y) * np.log(1-h)).mean()\n",
        "  \n",
        "  def fit(self,X,y):\n",
        "    if self.fit_intercept:\n",
        "      X = self.__add_intercept(X)\n",
        "    self.theta = np.zeros(X.shape[1])\n",
        "    \n",
        "    for i in range(self.num_iters):\n",
        "      z = np.dot(X,self.theta)\n",
        "      h = self.__sigmoid(z)\n",
        "      gradient = np.dot(X.T,(h-y))/y.size\n",
        "      \n",
        "      self.theta -= self.learning_rate * gradient\n",
        "      \n",
        "      z = np.dot(X,self.theta)\n",
        "      h = self.__sigmoid(z)\n",
        "      loss = self.__loss(h,y)\n",
        "      \n",
        "      if self.verbose == True and i % 1000 == 0:\n",
        "        print(f'Loss: {loss}\\t')\n",
        "  def predict_probability(self,X):\n",
        "    if self.fit_intercept:\n",
        "      X = self.__add_intercept(X)\n",
        "    return self.__sigmoid(np.dot(X,self.theta))\n",
        "  def predict(self,X):\n",
        "    return (self.predict_probability(X).round())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zj5_oG8-H_rM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#K means from scratch using oops\n",
        "class K_Means:\n",
        "    def __init__(self, k=2, tol=0.001, max_iter=300):\n",
        "        self.k = k\n",
        "        self.tol = tol\n",
        "        self.max_iter = max_iter\n",
        "\n",
        "    def fit(self,data):\n",
        "\n",
        "        self.centroids = {}\n",
        "\n",
        "        for i in range(self.k):\n",
        "            self.centroids[i] = data[i]\n",
        "\n",
        "        for i in range(self.max_iter):\n",
        "            self.classifications = {}\n",
        "\n",
        "            for i in range(self.k):\n",
        "                self.classifications[i] = []\n",
        "\n",
        "            for featureset in X:\n",
        "                distances = [np.linalg.norm(featureset-self.centroids[centroid]) for centroid in self.centroids]\n",
        "                classification = distances.index(min(distances))\n",
        "                self.classifications[classification].append(featureset)\n",
        "\n",
        "            prev_centroids = dict(self.centroids)\n",
        "\n",
        "            for classification in self.classifications:\n",
        "                self.centroids[classification] = np.average(self.classifications[classification],axis=0)\n",
        "\n",
        "            optimized = True\n",
        "\n",
        "            for c in self.centroids:\n",
        "                original_centroid = prev_centroids[c]\n",
        "                current_centroid = self.centroids[c]\n",
        "                if np.sum((current_centroid-original_centroid)/original_centroid*100.0) > self.tol:\n",
        "                    print(np.sum((current_centroid-original_centroid)/original_centroid*100.0))\n",
        "                    optimized = False\n",
        "\n",
        "            if optimized:\n",
        "                break\n",
        "\n",
        "    def predict(self,data):\n",
        "        distances = [np.linalg.norm(data-self.centroids[centroid]) for centroid in self.centroids]\n",
        "        classification = distances.index(min(distances))\n",
        "        return classification\n",
        "        \n",
        "colors = 10*[\"g\",\"r\",\"c\",\"b\",\"k\"]"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}